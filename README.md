This repository contains three distinct Python scripts designed to evaluate the performance of Meta-LLaMA language models across various benchmark datasets. Each script targets a specific dataset and employs different evaluation methodologies to provide comprehensive insights into the model's capabilities in diverse domains.

The first script, evaluate_ceval.py, assesses the Meta-LLaMA 3.1 8B Instruct Model using the CEVAL Exam dataset. It systematically iterates through a wide range of academic and professional tasks, generating predictions for each multiple-choice question and calculating the accuracy for each task. The results are aggregated and saved to an output file, enabling users to analyze the model's proficiency across different subjects such as Computer Networks, Advanced Mathematics, and Veterinary Medicine.

The second script, evaluate_squad_deepeval.py, focuses on evaluating the Meta-LLaMA 3.1 8B Model with the SQuAD dataset. Utilizing DeepEval's Answer Relevancy Metric, this script generates answers to questions from the SQuAD validation set and measures the relevancy of the responses compared to the ground truth. This evaluation provides a deeper understanding of the model's ability to comprehend and accurately respond to complex queries in a question-answering context.

The third script, evaluate_tmmluplus.py, tests the Meta-LLaMA 3.1 8B Model on the TMMLUPlus dataset, which encompasses an extensive array of over 60 tasks spanning various disciplines such as Engineering Math, Clinical Psychology, and Organic Chemistry. Similar to the first script, it generates predictions for each task's multiple-choice questions, computes the accuracy for each, and aggregates the results into an output file. This comprehensive evaluation highlights the model's strengths and areas for improvement across a broad spectrum of specialized subjects.
